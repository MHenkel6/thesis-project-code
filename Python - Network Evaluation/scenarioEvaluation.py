# -*- coding: utf-8 -*-
"""NetworkEvaluation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P0n8jtJrLj6_lRNGzYcn9mX2pGfK6a7F
"""

"""Package Import and Dependencies"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import io
import time
import shutil as sh
from pathlib import Path
import random
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support
from sklearn.utils.multiclass import unique_labels
from IPython.display import clear_output

root = 'D:/Files/TUDelftLocal/Thesis/Software/Evaluation/'
dataPath = root + "Data/TFRecord/"
modelPath = root + "Networks/"
outputPath = root + "Data/Scenarios/"


#%% Preprocessing Functions

def decode_TFRecord(exampleProto):
# Read TFRecord file
    # Define features
    featureDescription = {
        'x1': tf.VarLenFeature(dtype=tf.float32),
        'y1': tf.VarLenFeature(dtype=tf.float32),
        'z1': tf.VarLenFeature(dtype=tf.float32),
        'vx1': tf.VarLenFeature(dtype=tf.float32),
        'vy1': tf.VarLenFeature(dtype=tf.float32),
        'vz1':tf.VarLenFeature(dtype=tf.float32),
        'x2': tf.VarLenFeature(dtype=tf.float32),
        'y2': tf.VarLenFeature(dtype=tf.float32),
        'z2': tf.VarLenFeature(dtype=tf.float32),
        'vx2': tf.VarLenFeature(dtype=tf.float32),
        'vy2': tf.VarLenFeature(dtype=tf.float32),
        'vz2':tf.VarLenFeature(dtype=tf.float32),
        'x3': tf.VarLenFeature(dtype=tf.float32),
        'y3': tf.VarLenFeature(dtype=tf.float32),
        'z3': tf.VarLenFeature(dtype=tf.float32),
        'vx3': tf.VarLenFeature(dtype=tf.float32),
        'vy3': tf.VarLenFeature(dtype=tf.float32),
        'vz3': tf.VarLenFeature(dtype=tf.float32),
        'x4': tf.VarLenFeature(dtype=tf.float32),
        'y4': tf.VarLenFeature(dtype=tf.float32),
        'z4': tf.VarLenFeature(dtype=tf.float32),
        'vx4': tf.VarLenFeature(dtype=tf.float32),
        'vy4': tf.VarLenFeature(dtype=tf.float32),
        'vz4': tf.VarLenFeature(dtype=tf.float32),
        'label': tf.VarLenFeature(dtype=tf.int64),
        'time': tf.VarLenFeature(dtype=tf.int64),
        'sats': tf.VarLenFeature(dtype=tf.int64)}

    # Extract features from serialized data
    return  tf.io.parse_single_example(exampleProto, featureDescription)

def preprocDetInd(dataset,seqLen = 50,sat=0):
    #Set the Satellite viewpoint
    satView = tf.constant(sat,dtype=tf.int64)
    
    sats = tf.sparse.to_dense(dataset['sats'])
    labels = tf.sparse.to_dense(dataset['label'])
    time = tf.sparse.to_dense(dataset['time'])

    x1  = tf.sparse.to_dense(dataset['x1'])
    y1  = tf.sparse.to_dense(dataset['y1'])
    z1  = tf.sparse.to_dense(dataset['z1'])
    vx1 = tf.sparse.to_dense(dataset['vx1'])
    vy1 = tf.sparse.to_dense(dataset['vy1'])
    vz1 = tf.sparse.to_dense(dataset['vz1'])

    x2  = tf.sparse.to_dense(dataset['x2'])
    y2  = tf.sparse.to_dense(dataset['y2'])
    z2  = tf.sparse.to_dense(dataset['z2'])
    vx2 = tf.sparse.to_dense(dataset['vx2'])
    vy2 = tf.sparse.to_dense(dataset['vy2'])
    vz2 = tf.sparse.to_dense(dataset['vz2'])

    x3  = tf.sparse.to_dense(dataset['x3'])
    y3  = tf.sparse.to_dense(dataset['y3'])
    z3  = tf.sparse.to_dense(dataset['z3'])
    vx3 = tf.sparse.to_dense(dataset['vx3'])
    vy3 = tf.sparse.to_dense(dataset['vy3'])
    vz3 = tf.sparse.to_dense(dataset['vz3'])

    x4  = tf.sparse.to_dense(dataset['x4'])
    y4  = tf.sparse.to_dense(dataset['y4'])
    z4  = tf.sparse.to_dense(dataset['z4'])
    vx4 = tf.sparse.to_dense(dataset['vx4'])
    vy4 = tf.sparse.to_dense(dataset['vy4'])
    vz4 = tf.sparse.to_dense(dataset['vz4'])


    data = tf.stack([x1,y1,z1,vx1,vy1,vz1,
                     x2,y2,z2,vx2,vy2,vz2,
                     x3,y3,z3,vx3,vy3,vz3,
                     x4,y4,z4,vx4,vy4,vz4])
    data = tf.transpose(data)

    indices = tf.where((time[:-seqLen]<time[seqLen:]) & tf.equal(sats[:-seqLen],satView))
    zeros = tf.zeros_like(indices)
    begin = tf.stack([indices,zeros],axis = 1)
    begin = tf.reshape(begin,tf.shape(begin)[:2])
    # Construct dataset
    ds = tf.data.Dataset.from_tensor_slices(begin)

    # Map dataset as sequence of length seq_len and labels

    dataSlices = ds.map(lambda x: tf.slice(data,x,[seqLen,24]))
    correctLabels = tf.boolean_mask(labels,(time[:-seqLen]<time[seqLen:]) & tf.equal(sats[:-seqLen],satView))
    correctLabels = tf.reshape((correctLabels > 0),(-1,1))
    dataLabels = tf.data.Dataset.from_tensor_slices(correctLabels)

    ds3 = tf.data.Dataset.zip((dataSlices,dataLabels))
    return ds3

def preprocDet(dataset,seqLen = 50):
    #Set the Satellite viewpoint
    
    sats = tf.sparse.to_dense(dataset['sats'])
    labels = tf.sparse.to_dense(dataset['label'])
    time = tf.sparse.to_dense(dataset['time'])

    x1  = tf.sparse.to_dense(dataset['x1'])
    y1  = tf.sparse.to_dense(dataset['y1'])
    z1  = tf.sparse.to_dense(dataset['z1'])
    vx1 = tf.sparse.to_dense(dataset['vx1'])
    vy1 = tf.sparse.to_dense(dataset['vy1'])
    vz1 = tf.sparse.to_dense(dataset['vz1'])

    x2  = tf.sparse.to_dense(dataset['x2'])
    y2  = tf.sparse.to_dense(dataset['y2'])
    z2  = tf.sparse.to_dense(dataset['z2'])
    vx2 = tf.sparse.to_dense(dataset['vx2'])
    vy2 = tf.sparse.to_dense(dataset['vy2'])
    vz2 = tf.sparse.to_dense(dataset['vz2'])

    x3  = tf.sparse.to_dense(dataset['x3'])
    y3  = tf.sparse.to_dense(dataset['y3'])
    z3  = tf.sparse.to_dense(dataset['z3'])
    vx3 = tf.sparse.to_dense(dataset['vx3'])
    vy3 = tf.sparse.to_dense(dataset['vy3'])
    vz3 = tf.sparse.to_dense(dataset['vz3'])

    x4  = tf.sparse.to_dense(dataset['x4'])
    y4  = tf.sparse.to_dense(dataset['y4'])
    z4  = tf.sparse.to_dense(dataset['z4'])
    vx4 = tf.sparse.to_dense(dataset['vx4'])
    vy4 = tf.sparse.to_dense(dataset['vy4'])
    vz4 = tf.sparse.to_dense(dataset['vz4'])


    data = tf.stack([x1,y1,z1,vx1,vy1,vz1,
                     x2,y2,z2,vx2,vy2,vz2,
                     x3,y3,z3,vx3,vy3,vz3,
                     x4,y4,z4,vx4,vy4,vz4])
    data = tf.transpose(data)

    indices = tf.where((time[:-seqLen]<time[seqLen:]))
    zeros = tf.zeros_like(indices)
    begin = tf.stack([indices,zeros],axis = 1)
    begin = tf.reshape(begin,tf.shape(begin)[:2])
    # Construct dataset
    ds = tf.data.Dataset.from_tensor_slices(begin)

    # Map dataset as sequence of length seq_len and labels

    dataSlices = ds.map(lambda x: tf.slice(data,x,[seqLen,24]))
    
    correctLabels = tf.boolean_mask(labels,(time[:-seqLen]<time[seqLen:]))
    correctLabels = tf.reshape((correctLabels > 0),(-1,1))
    dataLabels = tf.data.Dataset.from_tensor_slices(correctLabels)

    ds3 = tf.data.Dataset.zip((dataSlices,dataLabels))
    return ds3

def preprocIso(dataset,seqLen = 50):
    labels = tf.sparse.to_dense(dataset['label'])
    time = tf.sparse.to_dense(dataset['time'])

    x1  = tf.sparse.to_dense(dataset['x1'])
    y1  = tf.sparse.to_dense(dataset['y1'])
    z1  = tf.sparse.to_dense(dataset['z1'])
    vx1 = tf.sparse.to_dense(dataset['vx1'])
    vy1 = tf.sparse.to_dense(dataset['vy1'])
    vz1 = tf.sparse.to_dense(dataset['vz1'])

    x2  = tf.sparse.to_dense(dataset['x2'])
    y2  = tf.sparse.to_dense(dataset['y2'])
    z2  = tf.sparse.to_dense(dataset['z2'])
    vx2 = tf.sparse.to_dense(dataset['vx2'])
    vy2 = tf.sparse.to_dense(dataset['vy2'])
    vz2 = tf.sparse.to_dense(dataset['vz2'])

    x3  = tf.sparse.to_dense(dataset['x3'])
    y3  = tf.sparse.to_dense(dataset['y3'])
    z3  = tf.sparse.to_dense(dataset['z3'])
    vx3 = tf.sparse.to_dense(dataset['vx3'])
    vy3 = tf.sparse.to_dense(dataset['vy3'])
    vz3 = tf.sparse.to_dense(dataset['vz3'])

    x4  = tf.sparse.to_dense(dataset['x4'])
    y4  = tf.sparse.to_dense(dataset['y4'])
    z4  = tf.sparse.to_dense(dataset['z4'])
    vx4 = tf.sparse.to_dense(dataset['vx4'])
    vy4 = tf.sparse.to_dense(dataset['vy4'])
    vz4 = tf.sparse.to_dense(dataset['vz4'])

    labels = tf.sparse.to_dense(dataset['label'])
    data = tf.stack([x1,y1,z1,vx1,vy1,vz1,
                     x2,y2,z2,vx2,vy2,vz2,
                     x3,y3,z3,vx3,vy3,vz3,
                     x4,y4,z4,vx4,vy4,vz4])
    data = tf.transpose(data)

    indices = tf.where(time[:-seqLen]<time[seqLen:])
    zeros = tf.zeros_like(indices)
    begin = tf.stack([indices,zeros],axis = 1)
    begin = tf.reshape(begin,tf.shape(begin)[:2])
    # Construct dataset
    ds = tf.data.Dataset.from_tensor_slices(begin)

    # Map dataset as sequence of length seq_len and labels

    dataSlices = ds.map(lambda x: tf.slice(data,x,[seqLen,24]))
    correctLabels =  tf.boolean_mask(labels,time[:-seqLen]<time[seqLen:])
    correctLabels = tf.one_hot(correctLabels,36,dtype=tf.int32)
    dataLabels = tf.data.Dataset.from_tensor_slices(correctLabels)

    ds3 = tf.data.Dataset.zip((dataSlices,dataLabels))
    return ds3

def preprocIsoInd(dataset,seqLen = 50,sat=0):
    #Set the Satellite viewpoint
    satView = tf.constant(sat,dtype=tf.int64)
    
    sats = tf.sparse.to_dense(dataset['sats'])
    labels = tf.sparse.to_dense(dataset['label'])
    time = tf.sparse.to_dense(dataset['time'])

    x1  = tf.sparse.to_dense(dataset['x1'])
    y1  = tf.sparse.to_dense(dataset['y1'])
    z1  = tf.sparse.to_dense(dataset['z1'])
    vx1 = tf.sparse.to_dense(dataset['vx1'])
    vy1 = tf.sparse.to_dense(dataset['vy1'])
    vz1 = tf.sparse.to_dense(dataset['vz1'])

    x2  = tf.sparse.to_dense(dataset['x2'])
    y2  = tf.sparse.to_dense(dataset['y2'])
    z2  = tf.sparse.to_dense(dataset['z2'])
    vx2 = tf.sparse.to_dense(dataset['vx2'])
    vy2 = tf.sparse.to_dense(dataset['vy2'])
    vz2 = tf.sparse.to_dense(dataset['vz2'])

    x3  = tf.sparse.to_dense(dataset['x3'])
    y3  = tf.sparse.to_dense(dataset['y3'])
    z3  = tf.sparse.to_dense(dataset['z3'])
    vx3 = tf.sparse.to_dense(dataset['vx3'])
    vy3 = tf.sparse.to_dense(dataset['vy3'])
    vz3 = tf.sparse.to_dense(dataset['vz3'])

    x4  = tf.sparse.to_dense(dataset['x4'])
    y4  = tf.sparse.to_dense(dataset['y4'])
    z4  = tf.sparse.to_dense(dataset['z4'])
    vx4 = tf.sparse.to_dense(dataset['vx4'])
    vy4 = tf.sparse.to_dense(dataset['vy4'])
    vz4 = tf.sparse.to_dense(dataset['vz4'])


    data = tf.stack([x1,y1,z1,vx1,vy1,vz1,
                     x2,y2,z2,vx2,vy2,vz2,
                     x3,y3,z3,vx3,vy3,vz3,
                     x4,y4,z4,vx4,vy4,vz4])
    data = tf.transpose(data)

    indices = tf.where((time[:-seqLen]<time[seqLen:]) & tf.equal(sats[:-seqLen],satView))
    zeros = tf.zeros_like(indices)
    begin = tf.stack([indices,zeros],axis = 1)
    begin = tf.reshape(begin,tf.shape(begin)[:2])
    # Construct dataset
    ds = tf.data.Dataset.from_tensor_slices(begin)

    # Map dataset as sequence of length seq_len and labels

    dataSlices = ds.map(lambda x: tf.slice(data,x,[seqLen,24]))
    
    correctLabels = tf.boolean_mask(labels,(time[:-seqLen]<time[seqLen:]) & tf.equal(sats[:-seqLen],satView))
    correctLabels = tf.one_hot(correctLabels,36,dtype=tf.int32)
    dataLabels = tf.data.Dataset.from_tensor_slices(correctLabels)

    ds3 = tf.data.Dataset.zip((dataSlices,dataLabels))
    return ds3

def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues, 
                         annotate=True):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    classes = [classes[x] for x in unique_labels(y_true, y_pred)]
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")
    fig.tight_layout()
    
    # Lines and size
    for pos in np.arange(0.5, len(classes)+0.5):
      ax.axhline(pos, linestyle='--')
      ax.axvline(pos, linestyle='--')
    fig.set_size_inches(15,15)
    
    if annotate:
      # Loop over data dimensions and create text annotations.
      fmt = '.2f' if normalize else 'd'
      thresh = cm.max() / 2.
      for i in range(cm.shape[0]):
          for j in range(cm.shape[1]):
              ax.text(j, i, format(cm[i, j], fmt),
                      ha="center", va="center",
                      color="white" if cm[i, j] > thresh else "black",
                     size=7)
    fig.tight_layout()
    return ax

def createDetectModel(seq_len=32, batch_size=None, stateful=True, 
              num_units=[32, 32]):
  source = tf.keras.Input(
  name='seed', shape=(seq_len, 24), 
      batch_size=batch_size)
  
  lstm_1 = tf.keras.layers.LSTM(num_units[0], stateful=stateful, return_sequences=True)(source)
  lstm_2 = tf.keras.layers.LSTM(num_units[1], stateful=stateful, return_sequences=False)(lstm_1)
  dense_1 = tf.keras.layers.Dense(64, activation='relu')(lstm_2)
  
  predict = tf.keras.layers.Dense(1, activation='sigmoid')(lstm_2)
  
  return tf.keras.Model(inputs=[source], outputs=[predict])

def createIsolateModel(seq_len=32, batch_size=None, stateful=True, 
              num_units=[32, 32]):
    source = tf.keras.Input(name='seed', shape=(seq_len, 24), 
                            batch_size=batch_size)
    lstm_1 = tf.keras.layers.LSTM(num_units[0], stateful=stateful, return_sequences=True,dropout=0.1, recurrent_dropout=0.1)(source)
    lstm_2 = tf.keras.layers.LSTM(num_units[1], stateful=stateful, return_sequences=False, dropout=0.1)(lstm_1)
    dense_1 = tf.keras.layers.Dense(100, activation='relu')(lstm_2)
    
    predict = tf.keras.layers.Dense(36, activation='softmax')(dense_1)
    
    return tf.keras.Model(inputs=[source], outputs=[predict])

#%% Loading Trained Networks

detFilesCreated = True
isoFilesCreated = True
debug = True
iso = False
det = False 
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
tf.enable_eager_execution()
# Available suffixes are "HighNoise", "LowThrust", "LowIntensity", "NavChange"
# and "LowIntensity"
suffix = "NavChangeV2"
detPath = dataPath + 'Detection/'
isoPath = dataPath + 'Isolation/'

# Parameter Definitions

TFinal = 5602
nSats = 6
# Define model parameters
nTimesteps = 50
if debug:
    batchSize = 4096
else:
    batchSize = 4096

# Detection Validation set
globDetailed = detPath + suffix + '_*.tfrecord'
listDetDetailed = tf.io.gfile.glob(globDetailed)
# Isolation Validation set
globDetailed = isoPath + suffix + '_*.tfrecord'
listIsoDetailed = tf.io.gfile.glob(globDetailed)

#%%  Determine Det beginning
last = True
det = []
fNames = []

#listDetDetailed = list(np.sort(listDetDetailed))

# with open(outputPath + "EvalDetNaive_HighNoise.csv",'r') as f:
#     lines = f.readlines()    
#     if not lines[-1][:4] == "Test":
#         fileName = lines[-2][:34]
#         last = False
#     else:
#         fileName = lines[-1][:34]
#     fNames.append(fileName)    
#     ind.append(listDetDetailed.index(detPath + fileName))

# openName = outputPath + "EvalDetInd0_HighNoise.csv"
# print(openName)
# with open(openName,'r') as f:
#     lines = f.readlines()    
#     if not lines[-1][:4] == "Test":
#         fileName = lines[-2][:34]
#         last = False
#     else:
#         fileName = lines[-1][:34]
#     print(fileName)
#     fNames.append(fileName)    
#     det.append(listDetDetailed.index(detPath + fileName))
# counter = min(ind)
# print(counter)
print(listDetDetailed)

# Load Detection Networks
detectNaive = tf.keras.models.load_model(modelPath + 'NaiveDetect_weights.02-0.83.hdf5')
detectInd0 = tf.keras.models.load_model(modelPath + 'detectInd0C_weights.03-0.86.hdf5')#'detectInd0_weights.06-0.86.hdf5'

#%% Make datasets of individual detection runs
evalNaiveData = []
evalInd0Data = []
evalIndoStateData = []
detNaiveCorrectedModel = createDetectModel(seq_len=50, batch_size=6*5552, stateful=True, num_units=[128, 128])
detNaiveCorrectedModel.set_weights(detectNaive.get_weights())

detInd0Model = createDetectModel(seq_len=50, batch_size=5552, stateful=True, num_units=[128, 128])

tS = time.perf_counter()
durAverage = 0
nFiles = len(listDetDetailed)
counter = 0
if not detFilesCreated :
    with open(outputPath + "EvalDetectNaive_"+ suffix +".csv",mode = 'wb') as (evalDN
      ), open(outputPath + "EvalDetectInd0_"+ suffix +".csv",mode = 'wb') as evalDI0:
        for i,fileName in enumerate(listDetDetailed):
            fName = fileName[len(dataPath)+10:]
            fParam = float(fName[-19:-9])
            fSat = int(fName[len(suffix)+3:len(suffix)+4])
            fThr = int(fName[len(suffix)+4:len(suffix)+5])
            fType = int(fName[len(suffix)+1:len(suffix)+2])
            fileListDet = tf.data.TFRecordDataset(fileName)
            decodeDetDataset = fileListDet.map(decode_TFRecord)
            processedDet = decodeDetDataset.flat_map(preprocDet)
            processedDetInd0 = decodeDetDataset.flat_map(lambda x: preprocDetInd(x,50,0))
            
            detDataset = processedDet.batch(batchSize,drop_remainder = False)
            detDataset0 = processedDetInd0.batch(batchSize,drop_remainder = False)

            detSteps = 9 
            #evalDetectNaiveFile = detectNaive.predict(detDataset,steps = detSteps,verbose = 1)
            evalDetectNaiveFile = detNaiveCorrectedModel.predict(processedDet.batch(batch_size=6*5552),steps = 1,verbose = 0)
            evalDetectInd0File = detectInd0.predict(detDataset0,steps = np.ceil(detSteps/6),verbose = 0)
            #evalDetectInd0StatefulFile = detInd0Model.predict(processedDetInd0.batch(batch_size=5552),steps =  1,verbose = 0)

            yTrueDetNaiveStack = [labels.numpy().astype(float) for values,labels in detDataset.take(-1)]   
            yTrueDetNaive = np.concatenate([item for sublist in yTrueDetNaiveStack for item in sublist]).reshape(-1,1)

            yTrueInd0Stack = [labels.numpy().astype(float) for values,labels in detDataset0.take(-1)]  
            yTrueInd0 = np.concatenate([item for sublist in yTrueInd0Stack for item in sublist]).reshape(-1,1)

            update = fName + ',' + str(fSat) + ',' + str(fThr) + ',' + str(fType) + ',' + str(fParam) + '\n'
            evalDN.write(update.encode('utf-8'))
            np.savetxt(evalDN, evalDetectNaiveFile.transpose())
            np.savetxt(evalDN, yTrueDetNaive.transpose())

            evalDI0.write(update.encode('utf-8'))
            np.savetxt(evalDI0, evalDetectInd0File.transpose())
            np.savetxt(evalDI0, yTrueInd0.transpose())

            counter += 1
            tE = time.perf_counter()
            dur = tE-tS
            if durAverage == 0 :
                durAverage = dur
            else:
                durAverage = 0.9*durAverage +0.1*dur 
            tS = tE
            remainder = (nFiles-counter)*durAverage/60
            print("Average time per file: {} seconds".format(durAverage))
            print("Estimated minutes left: {}".format(remainder))
if detTimeTest:
    
#%% Evaluating Isolation Models

numUnits = [256,356]

# Load Isolation Networks
isolateNaive = tf.keras.models.load_model(modelPath + 'weights.10-0.03.hdf5')
isolateInd0 = tf.keras.models.load_model(modelPath + 'isolateInd0_weights.17-0.64.hdf5')
isolateInd1 = tf.keras.models.load_model(modelPath + 'isolateInd1_weights.15-0.64.hdf5')
isolateInd2 = tf.keras.models.load_model(modelPath + 'isolateInd2_weights.14-0.64.hdf5')
isolateInd3 = tf.keras.models.load_model(modelPath + 'isolateInd3_weights.13-0.63.hdf5')
isolateInd4 = tf.keras.models.load_model(modelPath + 'isolateInd4_weights.15-0.65.hdf5')
isolateInd5 = tf.keras.models.load_model(modelPath + 'isolateInd5_weights.18-0.64.hdf5')

naiveweights = isolateNaive.get_weights()
ind0weights = isolateInd0.get_weights()
ind1weights = isolateInd1.get_weights()
ind2weights = isolateInd2.get_weights()
ind3weights = isolateInd3.get_weights()
ind4weights = isolateInd4.get_weights()
ind5weights = isolateInd5.get_weights()


bSizeEval = 512
# Define models to cope with changing data shape
isoNaiveModel = createIsolateModel(seq_len=50, batch_size=bSizeEval, stateful=True, num_units=[256,256])
isoInd0Model = createIsolateModel(seq_len=50, batch_size=bSizeEval, stateful=True, num_units=numUnits)
isoInd1Model = createIsolateModel(seq_len=50, batch_size=bSizeEval, stateful=True, num_units=numUnits)
isoInd2Model = createIsolateModel(seq_len=50, batch_size=bSizeEval, stateful=True, num_units=numUnits)
isoInd3Model = createIsolateModel(seq_len=50, batch_size=bSizeEval, stateful=True, num_units=numUnits)
isoInd4Model = createIsolateModel(seq_len=50, batch_size=bSizeEval, stateful=True, num_units=numUnits)
isoInd5Model = createIsolateModel(seq_len=50, batch_size=bSizeEval, stateful=True, num_units=numUnits)

isoNaiveModel.set_weights(naiveweights)
isoInd0Model.set_weights(ind0weights)
isoInd1Model.set_weights(ind1weights)
isoInd2Model.set_weights(ind2weights)
isoInd3Model.set_weights(ind3weights)
isoInd4Model.set_weights(ind4weights)
isoInd5Model.set_weights(ind5weights)

# Determine beginning
last = True
ind = []
fNames = []
listIsoDetailed = list(np.sort(listIsoDetailed))
# with open(outputPath + "EvalIsoNaive_HighNoise.csv",'r') as f:
#     lines = f.readlines()    
#     if not lines[-1][:4] == "Test":
#         fileName = lines[-2][:34]
#         last = False
#     else:
#         fileName = lines[-1][:34]
#     fNames.append(fileName)    
#     ind.append(listIsoDetailed.index(isoPath + fileName))
# for i in range(6):
#     openName = outputPath + "EvalIsoInd_HighNoise" +str(i)+".csv"
#     print(openName)
#     with open(openName,'r') as f:
#         lines = f.readlines()    
#         if not lines[-1][:4] == "Test":
#             fileName = lines[-2][:34]
#             last = False
#         else:
#             fileName = lines[-1][:34]
#         print(fileName)
#         fNames.append(fileName)    
#         ind.append(listIsoDetailed.index(isoPath + fileName))
# counter = min(ind)
counter = 0
print(counter)
print(listIsoDetailed)

#Load Confusion matrices
cmIso0 = np.load(isoPath + "EvalIsoInd_C0_CM.npy")
cmIso1 = np.load(isoPath + "EvalIsoInd_C1_CM.npy")
cmIso2 = np.load(isoPath + "EvalIsoInd_C2_CM.npy")
cmIso3 = np.load(isoPath + "EvalIsoInd_C3_CM.npy")
cmIso4 = np.load(isoPath + "EvalIsoInd_C4_CM.npy")
cmIso5 = np.load(isoPath + "EvalIsoInd_C5_CM.npy")
# Adjust off-diagonal elements
cmNorm0 = np.zeros([36,36])
cmNorm1 = np.zeros([36,36])
cmNorm2 = np.zeros([36,36])
cmNorm3 = np.zeros([36,36])
cmNorm4 = np.zeros([36,36])
cmNorm5 = np.zeros([36,36])

for j in range(36):
    colSum0 = np.sum(cmIso0[:,j])
    colSum1 = np.sum(cmIso1[:,j])
    colSum2 = np.sum(cmIso2[:,j])
    colSum3 = np.sum(cmIso3[:,j])
    colSum4 = np.sum(cmIso4[:,j])
    colSum5 = np.sum(cmIso5[:,j])
    for i in range(36):
        if not i == j:
            cmNorm0[i,j] = cmIso0[i,j] * -1/colSum0
            cmNorm1[i,j] = cmIso1[i,j] * -1/colSum1
            cmNorm2[i,j] = cmIso2[i,j] * -1/colSum2
            cmNorm3[i,j] = cmIso3[i,j] * -1/colSum3
            cmNorm4[i,j] = cmIso4[i,j] * -1/colSum4
            cmNorm5[i,j] = cmIso5[i,j] * -1/colSum5
        else:
            cmNorm0[i,j] = cmIso0[i,j] / colSum0
            cmNorm1[i,j] = cmIso1[i,j] / colSum1
            cmNorm2[i,j] = cmIso2[i,j] / colSum2
            cmNorm3[i,j] = cmIso3[i,j] / colSum3
            cmNorm4[i,j] = cmIso4[i,j] / colSum4
            cmNorm5[i,j] = cmIso5[i,j] / colSum5

#%% Make datasets of individual isolation runs
durAverage = 0
nFiles = len(listIsoDetailed)
dataSize = 5602*6#len(yTrueIsoNaive)
dataSizeInd = dataSize//6
tS = time.perf_counter()
counter = 0
# open(outputPath + "EvalIsoNaive_"+ suffix +".csv",mode = 'wb') as (evalIN
        # ), open(outputPath + "EvalIsoInd0_"+ suffix +".csv",mode = 'wb') as (evalII0
        # ), open(outputPath + "EvalIsoInd1_"+ suffix +".csv",mode = 'wb') as (evalII1
        # ), open(outputPath + "EvalIsoInd2_"+ suffix +".csv",mode = 'wb') as (evalII2
        # ), open(outputPath + "EvalIsoInd3_"+ suffix +".csv",mode = 'wb') as (evalII3 
        # ), open(outputPath + "EvalIsoInd4_"+ suffix +".csv",mode = 'wb') as (evalII4
        # ), open(outputPath + "EvalIsoInd5_"+ suffix +".csv",mode = 'wb') as (evalII5
        # ),
        
        #), open(outputPath + "CounterFile_"+ suffix +".csv",mode = 'wb') as counterFile 
    # ),# open(outputPath + "EvalIsoIndCombined.csv",mode = 'wb') as (evalIIC):
if not isoFilesCreated:
    with  open(outputPath + "EvalIsoInd0_"+ suffix +".csv",mode = 'wb') as (evalII0
        ), open(outputPath + "EvalIsoInd1_"+ suffix +".csv",mode = 'wb') as (evalII1
        ), open(outputPath + "EvalIsoInd2_"+ suffix +".csv",mode = 'wb') as (evalII2
        ), open(outputPath + "EvalIsoInd3_"+ suffix +".csv",mode = 'wb') as (evalII3 
        ), open(outputPath + "EvalIsoInd4_"+ suffix +".csv",mode = 'wb') as (evalII4
        ), open(outputPath + "EvalIsoInd5_"+ suffix +".csv",mode = 'wb') as (evalII5
        ), open(outputPath + "EvalIsoIndCombined_"+ suffix +".csv",mode = 'wb') as (evalIIC):
        for i,fileName in enumerate(listIsoDetailed[counter:]):
            fName = fileName[len(dataPath)+10:]
            fParam = float(fName[-19:-9])
            fSat = int(fName[len(suffix)+3:len(suffix)+4])
            fThr = int(fName[len(suffix)+4:len(suffix)+5])
            fType = int(fName[len(suffix)+1:len(suffix)+2])
            fileListIso = tf.data.TFRecordDataset(fileName)
            decodeIsoDataset = fileListIso.map(decode_TFRecord)
            #processedIso = decodeIsoDataset.flat_map(preprocIso)

            processedIsoInd0 = decodeIsoDataset.flat_map(lambda x: preprocIsoInd(x,50,0))
            processedIsoInd1 = decodeIsoDataset.flat_map(lambda x: preprocIsoInd(x,50,1))
            processedIsoInd2 = decodeIsoDataset.flat_map(lambda x: preprocIsoInd(x,50,2))
            processedIsoInd3 = decodeIsoDataset.flat_map(lambda x: preprocIsoInd(x,50,3))
            processedIsoInd4 = decodeIsoDataset.flat_map(lambda x: preprocIsoInd(x,50,4))
            processedIsoInd5 = decodeIsoDataset.flat_map(lambda x: preprocIsoInd(x,50,5))
            # yTrueIsoNaive = [labels for _,labels in processedIso.take(-1)]


            #isoDataset  = processedIso.batch(bSizeEval,drop_remainder = True)
            isoDataset0 = processedIsoInd0.batch(bSizeEval,drop_remainder = True)
            isoDataset1 = processedIsoInd1.batch(bSizeEval,drop_remainder = True)
            isoDataset2 = processedIsoInd2.batch(bSizeEval,drop_remainder = True)
            isoDataset3 = processedIsoInd3.batch(bSizeEval,drop_remainder = True)
            isoDataset4 = processedIsoInd4.batch(bSizeEval,drop_remainder = True)
            isoDataset5 = processedIsoInd5.batch(bSizeEval,drop_remainder = True)
            
            #evalIsolateNaive = isoNaiveModel.predict(isoDataset,steps = np.floor(dataSize/bSizeEval),verbose = 0)
            evalIsolateInd0 = isoInd0Model.predict(isoDataset0,steps = np.floor(dataSizeInd/bSizeEval),verbose = 0)
            evalIsolateInd1 = isoInd1Model.predict(isoDataset1,steps = np.floor(dataSizeInd/bSizeEval),verbose = 0)
            evalIsolateInd2 = isoInd2Model.predict(isoDataset2,steps = np.floor(dataSizeInd/bSizeEval),verbose = 0)
            evalIsolateInd3 = isoInd3Model.predict(isoDataset3,steps = np.floor(dataSizeInd/bSizeEval),verbose = 0)
            evalIsolateInd4 = isoInd4Model.predict(isoDataset4,steps = np.floor(dataSizeInd/bSizeEval),verbose = 0)
            evalIsolateInd5 = isoInd5Model.predict(isoDataset5,steps = np.floor(dataSizeInd/bSizeEval),verbose = 0)
            evalIsolateCombined = np.transpose(np.matmul(cmNorm0, np.transpose(evalIsolateInd0)) + 
                                               np.matmul(cmNorm1, np.transpose(evalIsolateInd1)) + 
                                               np.matmul(cmNorm2, np.transpose(evalIsolateInd2)) + 
                                               np.matmul(cmNorm3, np.transpose(evalIsolateInd3)) + 
                                               np.matmul(cmNorm4, np.transpose(evalIsolateInd4)) + 
                                               np.matmul(cmNorm5, np.transpose(evalIsolateInd5)))
            if suffix == "MultiFail":
                #evalIsolateNaiveFile = evalIsolateNaive
                evalIsolateInd0File = evalIsolateInd0
                evalIsolateInd1File = evalIsolateInd1
                evalIsolateInd2File = evalIsolateInd2
                evalIsolateInd3File = evalIsolateInd3
                evalIsolateInd4File = evalIsolateInd4
                evalIsolateInd5File = evalIsolateInd5
                evalIsolateCombinedFile = evalIsolateCombined
                saveFormat = '%.5e'
            else:
                #evalIsolateNaiveFile = np.argmax(evalIsolateNaive, axis = 1).reshape(-1,1).transpose()
                evalIsolateInd0File = np.argmax(evalIsolateInd0, axis = 1).reshape(-1,1).transpose()
                evalIsolateInd1File = np.argmax(evalIsolateInd1, axis = 1).reshape(-1,1).transpose()
                evalIsolateInd2File = np.argmax(evalIsolateInd2, axis = 1).reshape(-1,1).transpose()
                evalIsolateInd3File = np.argmax(evalIsolateInd3, axis = 1).reshape(-1,1).transpose()
                evalIsolateInd4File = np.argmax(evalIsolateInd4, axis = 1).reshape(-1,1).transpose()
                evalIsolateInd5File = np.argmax(evalIsolateInd5, axis = 1).reshape(-1,1).transpose()
                evalIsolateCombinedFile = np.argmax(evalIsolateCombined, axis = 1).reshape(-1,1).transpose()
                saveFormat = '%i'
                
            update = fName + ',' + str(fSat) + ',' + str(fThr) + ',' + str(fType) + ',' + str(fParam) + '\n'
            
            # evalIN.write(update.encode('utf-8'))
            # np.savetxt(evalIN, evalIsolateNaiveFile,fmt = saveFormat)

            evalII0.write(update.encode('utf-8'))
            np.savetxt(evalII0, evalIsolateInd0File,fmt = saveFormat)

            evalII1.write(update.encode('utf-8'))
            np.savetxt(evalII1, evalIsolateInd1File,fmt = saveFormat)

            evalII2.write(update.encode('utf-8'))
            np.savetxt(evalII2, evalIsolateInd2File,fmt = saveFormat)

            evalII3.write(update.encode('utf-8'))
            np.savetxt(evalII3, evalIsolateInd3File,fmt = saveFormat)

            evalII4.write(update.encode('utf-8'))
            np.savetxt(evalII4, evalIsolateInd4File,fmt = saveFormat)

            evalII5.write(update.encode('utf-8'))
            np.savetxt(evalII5, evalIsolateInd5File,fmt = saveFormat)
            
            evalIIC.write(update.encode('utf-8'))
            np.savetxt(evalIIC, evalIsolateCombinedFile,fmt = saveFormat)
            updateCounter = fName + ',' +str(counter) + '\n'

            # counterFile.write((updateCounter).encode('utf-8'))
            counter += 1
            if counter%10 == 0 :
                clear_output()
            tE = time.perf_counter()
            dur = tE-tS
            if durAverage == 0 :
                durAverage = dur
            else:
                durAverage = 0.9*durAverage +0.1*dur 
            tS = tE
            remainder = (nFiles-counter)*durAverage/60
            print("Average time per file: {} seconds".format(durAverage))
            print("Estimated minutes left: {}".format(remainder))
