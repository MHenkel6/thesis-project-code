{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Naive_IsolationNetwork_Training.ipynb","provenance":[],"collapsed_sections":["XXPq-8LWU97c"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XXPq-8LWU97c"},"source":["#  Naive Isolation Network Training Script\n","This script contains the code necessary to train the so called \"naive\" fault isolation network"]},{"cell_type":"markdown","metadata":{"id":"s5T4d4UoVGdq"},"source":["# Setup\n","The script requires tensorflow version 1.15.0"]},{"cell_type":"code","metadata":{"id":"EJp8h6c9dhGt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606893785135,"user_tz":-60,"elapsed":89956,"user":{"displayName":"Martin Henkel","photoUrl":"","userId":"16173242434936149558"}},"outputId":"4b3d75bc-16eb-4864-92e0-9d23bd22c5a5"},"source":["!pip install tensorflow==1.15.0"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n","\u001b[K     |████████████████████████████████| 412.3MB 36kB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.18.5)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.35.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.33.2)\n","Collecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 47.8MB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.3.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.12.4)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.10.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.15.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.2.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.2)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 60.4MB/s \n","\u001b[?25hCollecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n","\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.8.1)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0) (50.3.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.3)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (2.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.0)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=f2a0fc73fc38830be720d9c3092ed1d39cef5d1b0b9fbe0f9d83738ae0764bd0\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: tensorflow-estimator, tensorboard, gast, keras-applications, tensorflow\n","  Found existing installation: tensorflow-estimator 2.3.0\n","    Uninstalling tensorflow-estimator-2.3.0:\n","      Successfully uninstalled tensorflow-estimator-2.3.0\n","  Found existing installation: tensorboard 2.3.0\n","    Uninstalling tensorboard-2.3.0:\n","      Successfully uninstalled tensorboard-2.3.0\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorflow 2.3.0\n","    Uninstalling tensorflow-2.3.0:\n","      Successfully uninstalled tensorflow-2.3.0\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"48mgNcg8u39j"},"source":["Package Import and Dependencies"]},{"cell_type":"code","metadata":{"id":"M-byfdGQkQWW","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1593413603310,"user_tz":-120,"elapsed":189594,"user":{"displayName":"Martin Henkel","photoUrl":"","userId":"16173242434936149558"}},"outputId":"3b03d8dc-20ea-4404-9d38-9f439716b621"},"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import io\n","import os\n","import pickle\n","from pathlib import Path\n","import random\n","from collections import Counter\n","# Authentication for Managing Data\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","rootPath = '/content/drive/My Drive/Data/'\n","name = \"isolateNaive2Layer\"\n","stateful = False # boolea to toggle network state memory\n","debug = False # boolean to toggle debug options\n","distribute = False # boolean to toggle distribution of training (not functional)\n","loadModel = False # boolean to toggle loading a saved model\n","\n","if debug:\n","    tf.enable_eager_execution()\n","\n","if loadModel:\n","    checkpointPath = rootPath + 'Results/Isolation/'\n","    weightsName = 'weights.07-0.03.hdf5'\n","startEpoch = 1\n","#resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n","#tf.config.experimental_connect_to_cluster(resolver)\n","#tf.tpu.experimental.initialize_tpu_system(resolver)\n","#config = tf.ConfigProto()\n","#config.gpu_options.allow_growth = True\n","#sess = tf.Session(config=config)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QLkVXRJPVPBZ"},"source":["Properly mount the drive by reading a file from the drive"]},{"cell_type":"code","metadata":{"id":"FbcwhLDq68N_","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593413606285,"user_tz":-120,"elapsed":192559,"user":{"displayName":"Martin Henkel","photoUrl":"","userId":"16173242434936149558"}},"outputId":"9931a811-e5a2-42f0-9bd2-3f3320445850"},"source":["register = np.zeros(1)\n","while not np.any(register):\n","    try:\n","        with open(rootPath + 'Isolation/Training/FileRegister.csv','r') as f:\n","            register = np.genfromtxt(f,delimiter = \",\")\n","    except:\n","        pass\n","np.shape(register)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(97, 3)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"jWC5DJm2u-Ei"},"source":["Preprocessing Functions\n"]},{"cell_type":"code","metadata":{"id":"GqYolrfcuwZ_"},"source":["def decode_TFRecord(exampleProto):\n","    # Read TFRecord file\n","    # Define features\n","    featureDescription = {\n","        'x1': tf.VarLenFeature(dtype=tf.float32),\n","        'y1': tf.VarLenFeature(dtype=tf.float32),\n","        'z1': tf.VarLenFeature(dtype=tf.float32),\n","        'vx1': tf.VarLenFeature(dtype=tf.float32),\n","        'vy1': tf.VarLenFeature(dtype=tf.float32),\n","        'vz1':tf.VarLenFeature(dtype=tf.float32),\n","        'x2': tf.VarLenFeature(dtype=tf.float32),\n","        'y2': tf.VarLenFeature(dtype=tf.float32),\n","        'z2': tf.VarLenFeature(dtype=tf.float32),\n","        'vx2': tf.VarLenFeature(dtype=tf.float32),\n","        'vy2': tf.VarLenFeature(dtype=tf.float32),\n","        'vz2':tf.VarLenFeature(dtype=tf.float32),\n","        'x3': tf.VarLenFeature(dtype=tf.float32),\n","        'y3': tf.VarLenFeature(dtype=tf.float32),\n","        'z3': tf.VarLenFeature(dtype=tf.float32),\n","        'vx3': tf.VarLenFeature(dtype=tf.float32),\n","        'vy3': tf.VarLenFeature(dtype=tf.float32),\n","        'vz3': tf.VarLenFeature(dtype=tf.float32),\n","        'x4': tf.VarLenFeature(dtype=tf.float32),\n","        'y4': tf.VarLenFeature(dtype=tf.float32),\n","        'z4': tf.VarLenFeature(dtype=tf.float32),\n","        'vx4': tf.VarLenFeature(dtype=tf.float32),\n","        'vy4': tf.VarLenFeature(dtype=tf.float32),\n","        'vz4': tf.VarLenFeature(dtype=tf.float32),\n","        'label': tf.VarLenFeature(dtype=tf.int64),\n","        'time': tf.VarLenFeature(dtype=tf.int64),\n","        'sats': tf.VarLenFeature(dtype=tf.int64)}\n","\n","    # Extract features from serialized data\n","    return  tf.io.parse_single_example(exampleProto, featureDescription)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z635MSlBEH5w"},"source":["def preprocess(dataset,seqLen = 50):\n","    labels = tf.sparse.to_dense(dataset['label'])\n","    time = tf.sparse.to_dense(dataset['time'])\n","\n","    x1  = tf.sparse.to_dense(dataset['x1'])\n","    y1  = tf.sparse.to_dense(dataset['y1'])\n","    z1  = tf.sparse.to_dense(dataset['z1'])\n","    vx1 = tf.sparse.to_dense(dataset['vx1'])\n","    vy1 = tf.sparse.to_dense(dataset['vy1'])\n","    vz1 = tf.sparse.to_dense(dataset['vz1'])\n","\n","    x2  = tf.sparse.to_dense(dataset['x2'])\n","    y2  = tf.sparse.to_dense(dataset['y2'])\n","    z2  = tf.sparse.to_dense(dataset['z2'])\n","    vx2 = tf.sparse.to_dense(dataset['vx2'])\n","    vy2 = tf.sparse.to_dense(dataset['vy2'])\n","    vz2 = tf.sparse.to_dense(dataset['vz2'])\n","\n","    x3  = tf.sparse.to_dense(dataset['x3'])\n","    y3  = tf.sparse.to_dense(dataset['y3'])\n","    z3  = tf.sparse.to_dense(dataset['z3'])\n","    vx3 = tf.sparse.to_dense(dataset['vx3'])\n","    vy3 = tf.sparse.to_dense(dataset['vy3'])\n","    vz3 = tf.sparse.to_dense(dataset['vz3'])\n","\n","    x4  = tf.sparse.to_dense(dataset['x4'])\n","    y4  = tf.sparse.to_dense(dataset['y4'])\n","    z4  = tf.sparse.to_dense(dataset['z4'])\n","    vx4 = tf.sparse.to_dense(dataset['vx4'])\n","    vy4 = tf.sparse.to_dense(dataset['vy4'])\n","    vz4 = tf.sparse.to_dense(dataset['vz4'])\n","\n","    data = tf.stack([x1,y1,z1,vx1,vy1,vz1,\n","                     x2,y2,z2,vx2,vy2,vz2,\n","                     x3,y3,z3,vx3,vy3,vz3,\n","                     x4,y4,z4,vx4,vy4,vz4])\n","    data = tf.transpose(data)\n","    # take indices only the indices where the time at the beginning of a slice\n","    # is less than the time at the end of slice (ensures continuity)\n","    indices = tf.where(time[:-seqLen]<time[seqLen:])\n","    zeros = tf.zeros_like(indices)\n","    # the indices need zeros in the second column\n","    begin = tf.stack([indices,zeros],axis = 1)\n","    begin = tf.reshape(begin,tf.shape(begin)[:2])\n","    # Construct dataset from the beginning indices\n","    dsBegin = tf.data.Dataset.from_tensor_slices(begin)\n","\n","    # Map dataset as sequence of length seq_len and labels\n","    dataSlices = dsBegin.map(lambda x: tf.slice(data,x,[seqLen,24]))\n","    # select labels for data in the same way\n","    correctLabels =  tf.boolean_mask(labels,time[:-seqLen]<time[seqLen:])\n","    # Correct labels is a vector with a single 1 (one-hot) in the position\n","    # of the faulty thruster\n","    correctLabels = tf.one_hot(correctLabels,36,dtype=tf.int32)\n","    dataLabels = tf.data.Dataset.from_tensor_slices(correctLabels)\n","\n","    dsReturn = tf.data.Dataset.zip((dataSlices,dataLabels))\n","    return dsReturn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ju8bipaXFi0T"},"source":["Model Definition Function"]},{"cell_type":"code","metadata":{"id":"xfk_KcmIIyuk"},"source":["def createIsolateModel(seq_len, batch_size, stateful, \n","              num_units):\n","  source = tf.keras.Input(\n","  name='seed', shape=(seq_len, 24), \n","      batch_size=batch_size)\n","  \n","  lstm_1 = tf.keras.layers.LSTM(num_units[0], stateful=stateful, return_sequences=True,dropout=0.0, recurrent_dropout=0.0)(source)\n","  lstm_2 = tf.keras.layers.LSTM(num_units[1], stateful=stateful, return_sequences=False, dropout=0.0)(lstm_1)\n","  #lstm_3 = tf.keras.layers.LSTM(num_units[2], stateful=stateful, return_sequences=False, dropout=0.0)(lstm_1)\n","  dense_1 = tf.keras.layers.Dense(100, activation='relu')(lstm_2)\n","  \n","  predict = tf.keras.layers.Dense(36, activation='softmax')(dense_1)\n","  \n","  return tf.keras.Model(inputs=[source], outputs=[predict])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cfm1QOs3nA3g"},"source":["# Training Options"]},{"cell_type":"code","metadata":{"id":"JPsHLLEUNZ7f"},"source":["# Load Datasets \n","# Parameter Definitions\n","\n","TFinal = 5602\n","nSats = 6\n","# Define model parameters\n","nUnits = [256,356]\n","nTimesteps = 50\n","nEpoch = 10\n","if debug:\n","    batchSize = 4096\n","else:\n","    batchSize = 4096\n","learningRate = 0.1\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MxB7fZwUEoQc"},"source":["# Compile Model\n","if distribute:\n","    # Create distributed strategy\n","    # topology = tf.contrib.distribute.initialize_tpu_system()\n","    #device_assignment = tf.contrib.tpu.DeviceAssignment(topology, core_assignment=tf.contrib.tpu.SINGLE_CORE_ASSIGNMENT)\n","    #tpu_strategy = tf.contrib.distribute.TPUStrategy(device_assignment=device_assignment)\n","    #strategy = tf.distribute.experimental.TPUStrategy(resolver)\n","    strategy = tf.distribute.MirroredStrategy()\n","    if loadModel:\n","        detectModel = tf.keras.models.load_model(checkpointPath+weightsName)\n","    else:\n","        with strategy.scope():\n","            detectModel = createIsolateModel(seq_len=nTimesteps, stateful=stateful, num_units=nUnits, batch_size=batchSize)\n","            adams = tf.keras.optimizers.Adam(learning_rate=learningRate)\n","            sgd = tf.keras.optimizers.SGD(momentum=0.006)\n","            detectModel.compile(optimizer=adams,\n","                            loss=tf.keras.losses.CategoricalCrossentropy(),\n","                            metrics=['categorical_accuracy']) # Compile with adam optimizer\n","else:\n","    if loadModel:\n","        detectModel = tf.keras.models.load_model(checkpointPath+weightsName)\n","    else:\n","        detectModel = createIsolateModel(seq_len=nTimesteps, stateful=stateful, num_units=nUnits, batch_size=batchSize)\n","    adams = tf.keras.optimizers.Adam(learning_rate=learningRate)\n","    detectModel.compile(optimizer=adams,\n","                        loss=tf.keras.losses.CategoricalCrossentropy(),\n","                        metrics=['categorical_accuracy']) # Compile with adam optimizer\n","tf.keras.utils.plot_model(\n","    detectModel, to_file=rootPath + 'naiveIsolateModel.png', show_shapes=True, show_layer_names=True,\n","    rankdir='LR', expand_nested=False, dpi=96)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XydMA-_fZqGI","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1593413650912,"user_tz":-120,"elapsed":237144,"user":{"displayName":"Martin Henkel","photoUrl":"","userId":"16173242434936149558"}},"outputId":"39bb66e2-a3f7-4e72-961d-163e87ce7c4c"},"source":["# Load Datasets\n","pathTrain = rootPath + 'Isolation/Training/'\n","listdirTrain = []\n","if debug:\n","    globTrain = pathTrain + 'TrainCorrected_[0].tfrecord'\n","    listdirTrain = tf.io.gfile.glob(globTrain)\n","else:\n","    globTrain = pathTrain + 'TrainCorrected_[0-5].tfrecord'\n","    listdirTrain = tf.io.gfile.glob(globTrain)\n","globTrain = pathTrain + 'TrainCorrected_[0-9].tfrecord'\n","# Create File List\n","listdirTrain = tf.io.gfile.glob(globTrain)\n","nFilesTrain = len(listdirTrain)\n","# Validation set\n","pathTest = rootPath + 'Isolation/Testing/'\n","if debug:\n","    globTest = pathTest + 'TestCorrected_0.tfrecord'\n","else:\n","    globTest = pathTest + 'TestCorrected_[0].tfrecord'\n","listdirTest = tf.io.gfile.glob(globTest)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['/content/drive/My Drive/Data/Isolation/Training/TrainCorrected_0.tfrecord', '/content/drive/My Drive/Data/Isolation/Training/TrainCorrected_1.tfrecord', '/content/drive/My Drive/Data/Isolation/Training/TrainCorrected_2.tfrecord', '/content/drive/My Drive/Data/Isolation/Training/TrainCorrected_3.tfrecord', '/content/drive/My Drive/Data/Isolation/Training/TrainCorrected_4.tfrecord', '/content/drive/My Drive/Data/Isolation/Training/TrainCorrected_5.tfrecord', '/content/drive/My Drive/Data/Isolation/Training/TrainCorrected_6.tfrecord', '/content/drive/My Drive/Data/Isolation/Training/TrainCorrected_7.tfrecord', '/content/drive/My Drive/Data/Isolation/Training/TrainCorrected_8.tfrecord', '/content/drive/My Drive/Data/Isolation/Training/TrainCorrected_9.tfrecord']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CtjVDx5EKuaH","colab":{"base_uri":"https://localhost:8080/","height":176},"executionInfo":{"status":"ok","timestamp":1593413652911,"user_tz":-120,"elapsed":239135,"user":{"displayName":"Martin Henkel","photoUrl":"","userId":"16173242434936149558"}},"outputId":"093411c5-1c3b-41de-f248-b363b88a9347"},"source":["# Batch, shuffle and repeat dataset\n","fileListDatasetTrain = tf.data.TFRecordDataset(listdirTrain)\n","decodedDataset = fileListDatasetTrain.map(decode_TFRecord).cache()\n","processedDataset = decodedDataset.flat_map(preprocess)\n","trainDataset = processedDataset\n","if not stateful:\n","    trainDataset = trainDataset.shuffle(10*batchSize)\n","trainDataset = trainDataset.repeat(nEpoch).batch(batchSize,drop_remainder = True).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","# Validation Dataset\n","nFilesTest = len(listdirTest)\n","fileListDatasetTest = tf.data.TFRecordDataset(listdirTest).cache()\n","decodeValDataset = fileListDatasetTest.map(decode_TFRecord)\n","processedVal = decodeValDataset.flat_map(preprocess)\n","validationDataset = processedVal.batch(batchSize,drop_remainder = True).prefetch(tf.data.experimental.AUTOTUNE)\n","# Determine steps per epoch\n","trainSteps = int(nFilesTrain*6*100*(TFinal-nTimesteps)/batchSize)\n","testSteps = int(nFilesTest*6*100*(TFinal-nTimesteps)/batchSize)\n","\n","print(\"Training Files: {}\".format(nFilesTrain))\n","print(\"Testing Files: {}\".format(nFilesTest))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n","\n","WARNING:tensorflow:From <ipython-input-6-27e3817bfcb3>:41: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","['/content/drive/My Drive/Data/Isolation/Testing/TestCorrected_0.tfrecord']\n","Training Files: 10\n","Testing Files: 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IJNUeFw9WSwo"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"zaomvcbJ9gbK"},"source":["# Checkpoint to save the model every two epochs\n","checkpoint = tf.keras.callbacks.ModelCheckpoint(rootPath + \"Results/Isolation/\"+name+\"_weights.{epoch:02d}-{categorical_accuracy:.2f}.hdf5\", \n","                                                monitor='categorical_accuracy', verbose=0, save_best_only=False, \n","                                                save_weights_only=False, mode='auto', save_freq = 4879)\n","# Stopper to stop training if loss does not improve three times in a row\n","stopper = tf.keras.callbacks.EarlyStopping(patience=3,monitor='val_loss')\n","VAL = True\n","history = detectModel.fit(trainDataset, \n","                             epochs=nEpoch, steps_per_epoch = trainSteps,\n","                            callbacks=[checkpoint,stopper], \n","                            initial_epoch = startEpoch-1,\n","                            validation_data= validationDataset if VAL else None,\n","                            validation_steps = testSteps if VAL else None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"13Wmxvn4WcEr"},"source":["# Saving to local and to google drive\n","detectModel.save(rootPath +'Results/Isolation/{0}.hdf5'.format(name), overwrite=True)\n","detectModel.save_weights(rootPath + 'Results/Isolation/weights_{0}.h5'.format(name), overwrite=True)\n","\n","# Saving the training history\n","with open(rootPath + 'Results/Isolation/trainHistoryDict.pkl', 'wb') as file_pi:\n","  pickle.dump(history.history, file_pi)"],"execution_count":null,"outputs":[]}]}