{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Naive_DetectionNetwork_Training.ipynb","provenance":[],"collapsed_sections":["-Hzjc723RsFz","Cfm1QOs3nA3g"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"txyoRXewRah2"},"source":["# Naive Detection Network Training Script\n","This script contains the code necessary to train the so called \"naive\" fault *detection* network"]},{"cell_type":"markdown","metadata":{"id":"-Hzjc723RsFz"},"source":["# Setup\n","The script requires tensorflow version 1.15.0"]},{"cell_type":"code","metadata":{"id":"EJp8h6c9dhGt","colab":{"base_uri":"https://localhost:8080/","height":906},"executionInfo":{"status":"ok","timestamp":1593497557457,"user_tz":-120,"elapsed":98768,"user":{"displayName":"Martin Henkel","photoUrl":"","userId":"16173242434936149558"}},"outputId":"486edee9-d1ca-4dbb-9de7-bb00671a8225"},"source":["!pip install tensorflow==1.15.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n","\u001b[K     |████████████████████████████████| 412.3MB 35kB/s \n","\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.8.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.9.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.2)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.10.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.1)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 63.4MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.34.2)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.2.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.0.8)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.18.5)\n","Collecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 57.3MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.30.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.2.1)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0) (47.3.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.2.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.1.0)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=07ca002c4749cef8d7a1049be1981f7e6323338651aaf09073795c4f247afb41\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: tensorboard, tensorflow-estimator, gast, tensorflow\n","  Found existing installation: tensorboard 2.2.2\n","    Uninstalling tensorboard-2.2.2:\n","      Successfully uninstalled tensorboard-2.2.2\n","  Found existing installation: tensorflow-estimator 2.2.0\n","    Uninstalling tensorflow-estimator-2.2.0:\n","      Successfully uninstalled tensorflow-estimator-2.2.0\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorflow 2.2.0\n","    Uninstalling tensorflow-2.2.0:\n","      Successfully uninstalled tensorflow-2.2.0\n","Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"48mgNcg8u39j"},"source":["Package Import and Dependencies"]},{"cell_type":"code","metadata":{"id":"M-byfdGQkQWW","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1593497669235,"user_tz":-120,"elapsed":210533,"user":{"displayName":"Martin Henkel","photoUrl":"","userId":"16173242434936149558"}},"outputId":"a9bb9937-6eb9-43b0-ffec-a9b0caf92873"},"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import io\n","import os\n","import pickle\n","from pathlib import Path\n","import random\n","# Authentication for Managing Data\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","trainPercent  = 2 # percentage of training data to use\n","rootPath = '/content/drive/My Drive/Data/'\n","name = \"NaiveDetect\"\n","stateful = False # boolea to toggle network state memory\n","debug = False # boolean to toggle debug options\n","distribute = False # boolean to toggle distribution of training (not functional)\n","loadModel = False # boolean to toggle loading a saved model\n","if debug:\n","    tf.enable_eager_execution()\n","if loadModel:\n","    checkpointPath = rootPath + 'Results/Detection/'\n","    weightsName = 'detectNetwork10Epoch.hdf5'\n","    startEpoch = 7\n","else:\n","    startEpoch = 1\n","nEpoch = 20 # max number of epochs"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"phm_xLkOTG9J"},"source":["Properly mount the drive by reading a file from the drive"]},{"cell_type":"code","metadata":{"id":"FbcwhLDq68N_","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593497675022,"user_tz":-120,"elapsed":216310,"user":{"displayName":"Martin Henkel","photoUrl":"","userId":"16173242434936149558"}},"outputId":"b1f94ecd-5af0-4a10-dd08-94ee9a5190e9"},"source":["register = np.zeros(1)\n","while not np.any(register):\n","    try:\n","        with open(rootPath + 'Detection/Training/FileRegister.csv','r') as f:\n","            register = np.genfromtxt(f,delimiter = \",\")\n","    except:\n","        pass\n","np.shape(register)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(644, 3)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"jWC5DJm2u-Ei"},"source":["Preprocessing Functions\n"]},{"cell_type":"code","metadata":{"id":"GqYolrfcuwZ_"},"source":["def decode_TFRecord(exampleProto):\n","    # Read TFRecord file\n","    # Define features\n","    featureDescription = {\n","        'x1': tf.VarLenFeature(dtype=tf.float32),\n","        'y1': tf.VarLenFeature(dtype=tf.float32),\n","        'z1': tf.VarLenFeature(dtype=tf.float32),\n","        'vx1': tf.VarLenFeature(dtype=tf.float32),\n","        'vy1': tf.VarLenFeature(dtype=tf.float32),\n","        'vz1':tf.VarLenFeature(dtype=tf.float32),\n","        'x2': tf.VarLenFeature(dtype=tf.float32),\n","        'y2': tf.VarLenFeature(dtype=tf.float32),\n","        'z2': tf.VarLenFeature(dtype=tf.float32),\n","        'vx2': tf.VarLenFeature(dtype=tf.float32),\n","        'vy2': tf.VarLenFeature(dtype=tf.float32),\n","        'vz2':tf.VarLenFeature(dtype=tf.float32),\n","        'x3': tf.VarLenFeature(dtype=tf.float32),\n","        'y3': tf.VarLenFeature(dtype=tf.float32),\n","        'z3': tf.VarLenFeature(dtype=tf.float32),\n","        'vx3': tf.VarLenFeature(dtype=tf.float32),\n","        'vy3': tf.VarLenFeature(dtype=tf.float32),\n","        'vz3': tf.VarLenFeature(dtype=tf.float32),\n","        'x4': tf.VarLenFeature(dtype=tf.float32),\n","        'y4': tf.VarLenFeature(dtype=tf.float32),\n","        'z4': tf.VarLenFeature(dtype=tf.float32),\n","        'vx4': tf.VarLenFeature(dtype=tf.float32),\n","        'vy4': tf.VarLenFeature(dtype=tf.float32),\n","        'vz4': tf.VarLenFeature(dtype=tf.float32),\n","        'label': tf.VarLenFeature(dtype=tf.int64),\n","        'time': tf.VarLenFeature(dtype=tf.int64),\n","        'sats': tf.VarLenFeature(dtype=tf.int64)}\n","\n","    # Extract features from serialized data\n","    return  tf.io.parse_single_example(exampleProto, featureDescription)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z635MSlBEH5w"},"source":["def preprocess(dataset,seqLen = 50):\n","    # take the dataset and return the properly labelled slices \n","    labels = tf.sparse.to_dense(dataset['label'])\n","    time = tf.sparse.to_dense(dataset['time'])\n","\n","    x1  = tf.sparse.to_dense(dataset['x1'])\n","    y1  = tf.sparse.to_dense(dataset['y1'])\n","    z1  = tf.sparse.to_dense(dataset['z1'])\n","    vx1 = tf.sparse.to_dense(dataset['vx1'])\n","    vy1 = tf.sparse.to_dense(dataset['vy1'])\n","    vz1 = tf.sparse.to_dense(dataset['vz1'])\n","\n","    x2  = tf.sparse.to_dense(dataset['x2'])\n","    y2  = tf.sparse.to_dense(dataset['y2'])\n","    z2  = tf.sparse.to_dense(dataset['z2'])\n","    vx2 = tf.sparse.to_dense(dataset['vx2'])\n","    vy2 = tf.sparse.to_dense(dataset['vy2'])\n","    vz2 = tf.sparse.to_dense(dataset['vz2'])\n","\n","    x3  = tf.sparse.to_dense(dataset['x3'])\n","    y3  = tf.sparse.to_dense(dataset['y3'])\n","    z3  = tf.sparse.to_dense(dataset['z3'])\n","    vx3 = tf.sparse.to_dense(dataset['vx3'])\n","    vy3 = tf.sparse.to_dense(dataset['vy3'])\n","    vz3 = tf.sparse.to_dense(dataset['vz3'])\n","\n","    x4  = tf.sparse.to_dense(dataset['x4'])\n","    y4  = tf.sparse.to_dense(dataset['y4'])\n","    z4  = tf.sparse.to_dense(dataset['z4'])\n","    vx4 = tf.sparse.to_dense(dataset['vx4'])\n","    vy4 = tf.sparse.to_dense(dataset['vy4'])\n","    vz4 = tf.sparse.to_dense(dataset['vz4'])\n","\n","    data = tf.stack([x1,y1,z1,vx1,vy1,vz1,\n","                     x2,y2,z2,vx2,vy2,vz2,\n","                     x3,y3,z3,vx3,vy3,vz3,\n","                     x4,y4,z4,vx4,vy4,vz4])\n","    data = tf.transpose(data)\n","    \n","    # take indices only the indices where the time at the beginning of a slice\n","    # is less than the time at the end of slice (ensures continuity)\n","    indices = tf.where(time[:-seqLen]<time[seqLen:])\n","    zeros = tf.zeros_like(indices)\n","    # the indices need zeros in the second column\n","    begin = tf.stack([indices,zeros],axis = 1)\n","    begin = tf.reshape(begin,tf.shape(begin)[:2])\n","    # Construct dataset from the beginning indices\n","    dsBegin = tf.data.Dataset.from_tensor_slices(begin)\n","\n","    # Map dataset as sequence of length seq_len and labels\n","    dataSlices = dsBegin.map(lambda x: tf.slice(data,x,[seqLen,24]))\n","\n","    # select labels for data in the same way\n","    correctLabels = tf.boolean_mask(labels,time[:-seqLen]<time[seqLen:])\n","    correctLabels = tf.reshape((correctLabels > 0),(-1,1))\n","    dataLabels = tf.data.Dataset.from_tensor_slices(correctLabels)\n","    # zip up data and labels to one conistent dataset\n","    dsReturn = tf.data.Dataset.zip((dataSlices,dataLabels))\n","    return dsReturn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ju8bipaXFi0T"},"source":["Model Definition Function"]},{"cell_type":"code","metadata":{"id":"xfk_KcmIIyuk"},"source":["def createDetectModel(seq_len=32, batch_size=None, stateful=True, \n","              num_units=[32, 32]):\n","  source = tf.keras.Input(\n","  name='seed', shape=(seq_len, 24), \n","      batch_size=batch_size)\n","  \n","  lstm_1 = tf.keras.layers.LSTM(num_units[0], stateful=stateful, return_sequences=True,dropout=0.1, recurrent_dropout=0.1)(source)\n","  lstm_2 = tf.keras.layers.LSTM(num_units[1], stateful=stateful, return_sequences=False, dropout=0.1)(lstm_1)\n","  dense_1 = tf.keras.layers.Dense(64, activation='relu')(lstm_2)\n","  \n","  predict = tf.keras.layers.Dense(1, activation='sigmoid')(lstm_2)\n","  \n","  return tf.keras.Model(inputs=[source], outputs=[predict])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uwcaaBCxSe8c"},"source":["# Training Options"]},{"cell_type":"code","metadata":{"id":"JPsHLLEUNZ7f"},"source":["# Parameter Definitions\n","TFinal = 5602\n","nSats = 6\n","# Define model parameters\n","nUnits = [128,128]\n","nTimesteps = 50\n","if debug:\n","    batchSize = 1024\n","else:\n","    batchSize = 4096\n","learningRate = 0.005\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MxB7fZwUEoQc","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"ok","timestamp":1593497675748,"user_tz":-120,"elapsed":217019,"user":{"displayName":"Martin Henkel","photoUrl":"","userId":"16173242434936149558"}},"outputId":"a921482d-bccc-40f0-b353-cb9336817600"},"source":["# Compile Model\n","if distribute:\n","    # Create distributed strategy\n","    # topology = tf.contrib.distribute.initialize_tpu_system()\n","    #device_assignment = tf.contrib.tpu.DeviceAssignment(topology, core_assignment=tf.contrib.tpu.SINGLE_CORE_ASSIGNMENT)\n","    #tpu_strategy = tf.contrib.distribute.TPUStrategy(device_assignment=device_assignment)\n","    #strategy = tf.distribute.experimental.TPUStrategy(resolver)\n","    strategy = tf.distribute.MirroredStrategy()\n","    if loadModel:\n","        detectModel = tf.keras.models.load_model(checkpointPath+weightsName)\n","    else:\n","        with strategy.scope():\n","            detectModel = createDetectModel(seq_len=nTimesteps, stateful=stateful, num_units=nUnits, batch_size=batchSize)\n","            adams = tf.keras.optimizers.Adam(learning_rate=learningRate)\n","            sgd = tf.keras.optimizers.SGD(momentum=0.006)\n","            detectModel.compile(optimizer=adams,\n","                            loss=tf.keras.losses.BinaryCrossentropy(),\n","                            metrics=['binary_accuracy']) # Compile with adam optimizer\n","else:\n","    if loadModel:\n","        detectModel = tf.keras.models.load_model(checkpointPath+weightsName)\n","    else:\n","        detectModel = createDetectModel(seq_len=nTimesteps, stateful=stateful, num_units=nUnits, batch_size=batchSize)\n","    adams = tf.keras.optimizers.Adam(learning_rate=learningRate)\n","    detectModel.compile(optimizer=adams,\n","                        loss=tf.keras.losses.BinaryCrossentropy(),\n","                        metrics=['binary_accuracy']) # Compile with adam optimizer\n","tf.keras.utils.plot_model(\n","    detectModel, to_file=rootPath + 'naiveDetectModel.png', show_shapes=True, show_layer_names=True,\n","    rankdir='LR', expand_nested=False, dpi=96)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XydMA-_fZqGI","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1593497694342,"user_tz":-120,"elapsed":235603,"user":{"displayName":"Martin Henkel","photoUrl":"","userId":"16173242434936149558"}},"outputId":"06e4996f-2a0b-4d7a-e463-c8d7dc619037"},"source":["#Load Datasets\n","pathTrain = rootPath + 'Detection/Training/'\n","listdirTrain = []\n","if debug:\n","    globTrain = pathTrain + 'Train_[0].tfrecord'\n","    listdirTrain = tf.io.gfile.glob(globTrain)\n","else:\n","    # Select the worst and best 5% of faults\n","    with open(pathTrain + 'FaultRegister.csv','r') as f:\n","        faultRegister = np.genfromtxt(f,delimiter = \",\")\n","    sortedReg = faultRegister[faultRegister[:,2].argsort()]\n","    nFiles = int(trainPercent/2 /100 * faultRegister[-1,0])\n","    fileIndices = np.concatenate([sortedReg[:nFiles,0].astype(int),sortedReg[-nFiles:,0].astype(int)])\n","    for index in fileIndices:\n","        listdirTrain.append(pathTrain + 'TrainCorrected_' + str(index) + '.tfrecord')\n","nFilesTrain = len(listdirTrain)\n","\n","# Validation set\n","pathTest = rootPath + 'Detection/Testing/'\n","if debug:\n","    globTest = pathTest + 'Test_0.tfrecord'\n","else:\n","    globTest = pathTest + 'Test_[0].tfrecord'\n","listdirTest = tf.io.gfile.glob(globTest)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['/content/drive/My Drive/Data/Detection/Training/TrainCorrected_478.tfrecord', '/content/drive/My Drive/Data/Detection/Training/TrainCorrected_499.tfrecord', '/content/drive/My Drive/Data/Detection/Training/TrainCorrected_20.tfrecord', '/content/drive/My Drive/Data/Detection/Training/TrainCorrected_205.tfrecord', '/content/drive/My Drive/Data/Detection/Training/TrainCorrected_326.tfrecord', '/content/drive/My Drive/Data/Detection/Training/TrainCorrected_383.tfrecord', '/content/drive/My Drive/Data/Detection/Training/TrainCorrected_330.tfrecord', '/content/drive/My Drive/Data/Detection/Training/TrainCorrected_223.tfrecord', '/content/drive/My Drive/Data/Detection/Training/TrainCorrected_68.tfrecord', '/content/drive/My Drive/Data/Detection/Training/TrainCorrected_324.tfrecord']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CtjVDx5EKuaH","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1593497696406,"user_tz":-120,"elapsed":237658,"user":{"displayName":"Martin Henkel","photoUrl":"","userId":"16173242434936149558"}},"outputId":"ae740651-99af-45a3-d857-5dc8a03fbbca"},"source":["# Batch, shuffle and repeat training dataset\n","fileListDatasetTrain = tf.data.TFRecordDataset(listdirTrain)\n","decodedDataset = fileListDatasetTrain.map(decode_TFRecord)\n","processedDataset = decodedDataset.flat_map(preprocess)\n","trainDataset = processedDataset\n","\n","if not stateful:\n","    trainDataset = trainDataset.shuffle(10*batchSize)\n","trainDataset = trainDataset.repeat(nEpoch).batch(batchSize,drop_remainder = True).prefetch(tf.data.experimental.AUTOTUNE)\n","nFilesTest = len(listdirTest)\n","\n","# Validation Dataset\n","fileListDatasetTest = tf.data.TFRecordDataset(listdirTest)\n","decodeValDataset = fileListDatasetTest.map(decode_TFRecord)\n","processedVal = decodeValDataset.flat_map(preprocess)\n","validationDataset = processedVal.batch(batchSize,drop_remainder = True).prefetch(tf.data.experimental.AUTOTUNE)\n","# Determine steps per epoch\n","trainSteps = int(nFilesTrain*6*100*(TFinal-nTimesteps)/batchSize)\n","testSteps = int(nFilesTest*6*100*(TFinal-nTimesteps)/batchSize)\n","\n","print(\"Training Files: {}\".format(nFilesTrain))\n","print(\"Testing Files: {}\".format(nFilesTest))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n","\n","['/content/drive/My Drive/Data/Detection/Testing/Test_0.tfrecord']\n","Training Files: 10\n","Testing Files: 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Cfm1QOs3nA3g"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"zaomvcbJ9gbK","colab":{"base_uri":"https://localhost:8080/","height":228},"executionInfo":{"status":"ok","timestamp":1593506451625,"user_tz":-120,"elapsed":8992864,"user":{"displayName":"Martin Henkel","photoUrl":"","userId":"16173242434936149558"}},"outputId":"00d6a626-4d12-4c72-8b81-bc2b3f75a6bb"},"source":["# Checkpoint to save the model every two epochs\n","checkpoint = tf.keras.callbacks.ModelCheckpoint(rootPath + \"Results/Detection/\"+name+\"_weights.{epoch:02d}-{binary_accuracy:.2f}.hdf5\", \n","                                                monitor='binary_accuracy', verbose=0, save_best_only=False, \n","                                                save_weights_only=False, mode='auto', save_freq = 8132)\n","# Stopper to stop training if loss does not improve three times in a row\n","stopper = tf.keras.callbacks.EarlyStopping(patience=3,monitor='val_loss')\n","VAL = True\n","history = detectModel.fit(trainDataset, \n","                             epochs=nEpoch, steps_per_epoch = trainSteps,\n","                            callbacks=[checkpoint,stopper], \n","                            initial_epoch = startEpoch-1,\n","                            validation_data= validationDataset if VAL else None,\n","                            validation_steps = testSteps if VAL else None)\n","#\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train on 8132 steps, validate on 813 steps\n","Epoch 1/20\n","8132/8132 [==============================] - 1776s 218ms/step - loss: 0.3929 - binary_accuracy: 0.8271 - val_loss: 0.6489 - val_binary_accuracy: 0.8159\n","Epoch 2/20\n","8132/8132 [==============================] - 1744s 214ms/step - loss: 0.3981 - binary_accuracy: 0.8314 - val_loss: 0.5560 - val_binary_accuracy: 0.8224\n","Epoch 3/20\n","8132/8132 [==============================] - 1743s 214ms/step - loss: 0.3882 - binary_accuracy: 0.8375 - val_loss: 0.6105 - val_binary_accuracy: 0.8251\n","Epoch 4/20\n","8132/8132 [==============================] - 1746s 215ms/step - loss: 0.3898 - binary_accuracy: 0.8350 - val_loss: 0.5609 - val_binary_accuracy: 0.8217\n","Epoch 5/20\n","8132/8132 [==============================] - 1743s 214ms/step - loss: 0.4056 - binary_accuracy: 0.8298 - val_loss: 0.9036 - val_binary_accuracy: 0.8030\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FVkeuyxWJwXM"},"source":["# Saving to local and to google drive\n","detectModel.save(rootPath +'Results/Detection/{0}.hdf5'.format(name), overwrite=True)\n","detectModel.save_weights(rootPath + 'Results/Detection/weights_{0}.h5'.format(name), overwrite=True)\n","\n","# Saving the training history\n","with open(rootPath + 'Results/Detection/trainHistoryDict{0}.pkl'.format(name), 'wb') as file_pi:\n","    pickle.dump(history.history, file_pi)"],"execution_count":null,"outputs":[]}]}